"""
è²è‘‰æ–¯åˆ†é¡å™¨æ»¾å‹•é©—è­‰ç³»çµ±
ä½¿ç”¨æ¨¸ç´ è²è‘‰æ–¯ä¾†åˆ†é¡ç‹€æ…‹ï¼Œé æ¸¬5å¤©å¾Œçš„å ±é…¬ç‡åˆ†é¡
å°ˆæ³¨æ–¼æ»¾å‹•é©—è­‰ï¼ˆWalk-Forward Validationï¼‰
å›æ¸¬ä½¿ç”¨éš”å¤©çš„é–‹ç›¤åƒ¹é€²è¡Œäº¤æ˜“
"""

import pandas as pd
import numpy as np
import json
import logging
import joblib
import warnings
from sklearn.naive_bayes import CategoricalNB, GaussianNB
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from pathlib import Path
from datetime import datetime
from tqdm import tqdm

warnings.filterwarnings('ignore')

class BayesianStateClassifier:
    """è²è‘‰æ–¯ç‹€æ…‹åˆ†é¡å™¨"""
    
    def __init__(self, lookback_days: int = 5):
        self.lookback_days = lookback_days
        self.model = CategoricalNB()
        self.feature_columns = []
        self.label_encoders = {}
        self.logger = self._setup_logger()
        
        # åˆ†é¡é–¾å€¼
        self.return_threshold_buy = 0.03   # 3%
        self.return_threshold_sell = -0.03  # -3%
        
        # åˆ†é¡æ¨™ç±¤
        self.class_labels = {'buy': 0, 'hold': 1, 'sell': 2}
        self.inverse_labels = {0: 'buy', 1: 'hold', 2: 'sell'}
        
    def _setup_logger(self) -> logging.Logger:
        """è¨­ç½®æ—¥èªŒ"""
        logger = logging.getLogger('BayesianClassifier')
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            # å‰µå»ºæ–‡ä»¶è™•ç†å™¨ï¼Œè¨­ç½®UTF-8ç·¨ç¢¼
            handler = logging.FileHandler('log/bayesian_classifier.log', encoding='utf-8')
            
            # è¨­ç½®æ›´è©³ç´°çš„æ—¥èªŒæ ¼å¼
            formatter = logging.Formatter(
                '%(asctime)s | %(levelname)-5s | %(funcName)-25s | %(message)s',
                datefmt='%Y-%m-%d %H:%M:%S'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            
            # æ§åˆ¶å°è™•ç†å™¨åªé¡¯ç¤ºé‡è¦ä¿¡æ¯
            console_handler = logging.StreamHandler()
            console_formatter = logging.Formatter(
                '%(asctime)s | %(levelname)-5s | %(message)s',
                datefmt='%H:%M:%S'
            )
            console_handler.setFormatter(console_formatter)
            console_handler.setLevel(logging.ERROR)  # æ§åˆ¶å°åªé¡¯ç¤ºéŒ¯èª¤
            logger.addHandler(console_handler)
            
        return logger
    
    def _calculate_future_return(self, data: pd.DataFrame, current_idx: int) -> float:
        """è¨ˆç®—æœªä¾†5å¤©çš„å ±é…¬ç‡"""
        if current_idx + self.lookback_days >= len(data):
            return None
        
        current_price = data.iloc[current_idx]['Close']
        future_price = data.iloc[current_idx + self.lookback_days]['Close']
        
        return (future_price - current_price) / current_price
    
    def _classify_return(self, return_value: float) -> str:
        """å°‡å ±é…¬ç‡åˆ†é¡ç‚ºè²·å…¥/è³£å‡º/æŒæœ‰"""
        if return_value > self.return_threshold_buy:
            return 'buy'
        elif return_value < self.return_threshold_sell:
            return 'sell'
        else:
            return 'hold'
    
    def _encode_features(self, X: pd.DataFrame, fit: bool = False) -> np.ndarray:
        """ç·¨ç¢¼åˆ†é¡ç‰¹å¾µ"""
        X_encoded = X.copy()
        
        for col in X.columns:
            if fit:
                self.label_encoders[col] = LabelEncoder()
                X_encoded[col] = self.label_encoders[col].fit_transform(X[col])
            else:
                X_encoded[col] = self.label_encoders[col].transform(X[col])
                
        
        return X_encoded.values
    
    def prepare_data(self, data: pd.DataFrame) -> tuple:
        """æº–å‚™è¨“ç·´æ•¸æ“š"""
        self.logger.info("æº–å‚™è¨“ç·´æ•¸æ“š...")
        
        # è­˜åˆ¥binç‰¹å¾µ
        self.feature_columns = [col for col in data.columns if col.endswith('_bin')]
        self.logger.info(f"ä½¿ç”¨ {len(self.feature_columns)} å€‹binç‰¹å¾µ: {self.feature_columns}")
        
        # æº–å‚™ç‰¹å¾µå’Œæ¨™ç±¤
        X_list = []
        y_list = []
        valid_indices = []
        
        for i in range(len(data) - self.lookback_days):
            future_return = self._calculate_future_return(data, i)
            if future_return is not None:
                X_list.append(data.iloc[i][self.feature_columns])
                y_list.append(self.class_labels[self._classify_return(future_return)])
                valid_indices.append(i)
                
        
        X = pd.DataFrame(X_list)
        y = np.array(y_list)
        
        self.logger.info(f"æº–å‚™äº† {len(X)} å€‹æœ‰æ•ˆæ¨£æœ¬")
        
        # çµ±è¨ˆæ¨™ç±¤åˆ†å¸ƒ
        unique, counts = np.unique(y, return_counts=True)
        for label_idx, count in zip(unique, counts):
            action = self.inverse_labels[label_idx]
            percentage = count / len(y) * 100
            self.logger.info(f"{action}: {count} æ¨£æœ¬ ({percentage:.1f}%)")
        
        return X, y, valid_indices
    
    def train(self, data: pd.DataFrame):
        """è¨“ç·´è²è‘‰æ–¯åˆ†é¡å™¨"""
        self.logger.info("=" * 50)
        self.logger.info("ğŸš€ é–‹å§‹è¨“ç·´è²è‘‰æ–¯åˆ†é¡å™¨...")
        self.logger.info("=" * 50)
        
        # æº–å‚™æ•¸æ“š
        self.logger.info("ğŸ“Š æº–å‚™è¨“ç·´æ•¸æ“š...")
        X, y, _ = self.prepare_data(data)
        
        # ç·¨ç¢¼ç‰¹å¾µ
        self.logger.info("ğŸ”§ ç·¨ç¢¼ç‰¹å¾µæ•¸æ“š...")
        X_encoded = self._encode_features(X, fit=True)
        
        # åˆ†å‰²æ•¸æ“š
        self.logger.info("âœ‚ï¸ åˆ†å‰²è¨“ç·´å’Œæ¸¬è©¦æ•¸æ“š...")
        X_train, X_test, y_train, y_test = train_test_split(
            X_encoded, y, test_size=0.2, random_state=42, stratify=y
        )
        
        self.logger.info(f"è¨“ç·´é›†: {len(X_train)} æ¨£æœ¬")
        self.logger.info(f"æ¸¬è©¦é›†: {len(X_test)} æ¨£æœ¬")
        
        # è¨“ç·´æ¨¡å‹
        self.model.fit(X_train, y_train)
        
        # è©•ä¼°æ¨¡å‹
        self.logger.info("ğŸ“ˆ è©•ä¼°æ¨¡å‹æ€§èƒ½...")
        train_score = self.model.score(X_train, y_train)
        test_score = self.model.score(X_test, y_test)
        
        self.logger.info(f"âœ… è¨“ç·´æº–ç¢ºç‡: {train_score:.4f}")
        self.logger.info(f"ğŸ¯ æ¸¬è©¦æº–ç¢ºç‡: {test_score:.4f}")
        
        # è©³ç´°è©•ä¼°
        y_pred = self.model.predict(X_test)
        
        # åˆ†é¡å ±å‘Š
        target_names = ['buy', 'hold', 'sell']
        class_report = classification_report(y_test, y_pred, target_names=target_names, output_dict=True)
        
        self.logger.info("ğŸ“Š å„å‹•ä½œåˆ†é¡æ€§èƒ½:")
        for action in target_names:
            metrics = class_report[action]
            self.logger.info(f"   {action:>4s}: ç²¾ç¢ºç‡={metrics['precision']:.3f}, å¬å›ç‡={metrics['recall']:.3f}, F1={metrics['f1-score']:.3f}")
        
        # æ··æ·†çŸ©é™£
        cm = confusion_matrix(y_test, y_pred)
        self.logger.info(f"ğŸ” æ··æ·†çŸ©é™£:\n{cm}")
        
        # äº¤å‰é©—è­‰
        self.logger.info("ğŸ”„ åŸ·è¡Œ5æŠ˜äº¤å‰é©—è­‰...")
        cv_scores = cross_val_score(self.model, X_encoded, y, cv=5)
        self.logger.info(f"ğŸ“Š 5æŠ˜äº¤å‰é©—è­‰æº–ç¢ºç‡: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")
        
        # ä¿å­˜è©•ä¼°çµæœ
        self.evaluation_results = {
            'train_accuracy': train_score,
            'test_accuracy': test_score,
            'cv_accuracy_mean': cv_scores.mean(),
            'cv_accuracy_std': cv_scores.std(),
            'classification_report': class_report,
            'confusion_matrix': cm.tolist()
        }
        
        self.logger.info("âœ… è¨“ç·´å®Œæˆ")
        self.logger.info("=" * 50)
        
        return self.evaluation_results
    
    def predict(self, features: pd.Series) -> dict:
        """é æ¸¬å–®å€‹æ¨£æœ¬"""
        # æº–å‚™ç‰¹å¾µ
        X = pd.DataFrame([features[self.feature_columns]])
        X_encoded = self._encode_features(X, fit=False)
        
        # é æ¸¬
        prediction = self.model.predict(X_encoded)[0]
        probabilities = self.model.predict_proba(X_encoded)[0]
        
        # è½‰æ›çµæœ
        predicted_action = self.inverse_labels[prediction]
        action_probabilities = {
            action: float(probabilities[idx]) 
            for idx, action in self.inverse_labels.items()
        }
        
        # è¨ˆç®—ä¿¡å¿ƒåº¦
        confidence = max(probabilities)
        
        return {
            'action': predicted_action,
            'confidence': confidence,
            'probabilities': action_probabilities,
            'prediction_index': int(prediction)
        }
    
    def predict_batch(self, data: pd.DataFrame) -> pd.DataFrame:
        """æ‰¹é‡é æ¸¬"""
        predictions = []
        
        for i in range(len(data)):
            try:
                result = self.predict(data.iloc[i])
                predictions.append(result)
            except Exception as e:
                self.logger.warning(f"é æ¸¬å¤±æ•— (ç¬¬{i}è¡Œ): {e}")
                predictions.append({'action': 'hold', 'confidence': 0.0, 'probabilities': {}})
                
        
        return pd.DataFrame(predictions)
    
    def _calculate_max_drawdown(self, portfolio_values: pd.Series) -> float:
        """è¨ˆç®—æœ€å¤§å›æ’¤"""
        if len(portfolio_values) == 0:
            return 0.0
        peak = portfolio_values.cummax()
        drawdown = (portfolio_values - peak) / peak
        return drawdown.min()
    
    def get_feature_importance(self) -> dict:
        """ç²å–ç‰¹å¾µé‡è¦æ€§ï¼ˆåŸºæ–¼æ¢ä»¶æ¦‚ç‡ï¼‰"""
        if not hasattr(self.model, 'feature_log_prob_'):
            return {}
        
        try:
            # è¨ˆç®—æ¯å€‹ç‰¹å¾µå°æ¯å€‹é¡åˆ¥çš„è²¢ç»
            feature_importance = {}
            
            # å°æ–¼CategoricalNBï¼Œfeature_log_prob_æ˜¯ä¸€å€‹åˆ—è¡¨
            for i, feature in enumerate(self.feature_columns):
                importance = 0
                for class_idx in range(len(self.class_labels)):
                    if i < len(self.model.feature_log_prob_[class_idx]):
                        # è¨ˆç®—è©²ç‰¹å¾µåœ¨è©²é¡åˆ¥ä¸­çš„å¹³å‡é‡è¦æ€§
                        class_importance = np.exp(self.model.feature_log_prob_[class_idx][i]).var()
                        importance += class_importance
                feature_importance[feature] = importance
            
            # æ’åº
            sorted_importance = dict(sorted(feature_importance.items(), key=lambda x: x[1], reverse=True))
            
            return sorted_importance
        except Exception as e:
            self.logger.warning(f"ç„¡æ³•è¨ˆç®—ç‰¹å¾µé‡è¦æ€§: {e}")
            # è¿”å›å¹³å‡é‡è¦æ€§
            return {feature: 1.0 for feature in self.feature_columns}
    
    def get_next_day_signal(self, current_data: pd.Series) -> dict:
        """æ ¹æ“šä»Šå¤©çš„æ•¸æ“šé æ¸¬æ˜å¤©çš„äº¤æ˜“è¨Šè™Ÿ"""
        prediction = self.predict(current_data)
        
        signal_info = {
            'signal_date': datetime.now().isoformat(),
            'for_date': (datetime.now() + pd.Timedelta(days=1)).isoformat(),
            'current_price': float(current_data['Close']),
            'recommended_action': prediction['action'],
            'confidence': prediction['confidence'],
            'action_probabilities': prediction['probabilities'],
            'reasoning': self._get_signal_reasoning(prediction)
        }
        
        return signal_info
    
    def _get_signal_reasoning(self, prediction: dict) -> str:
        """ç”Ÿæˆè¨Šè™Ÿæ¨ç†èªªæ˜"""
        action = prediction['action']
        confidence = prediction['confidence']
        
        if action == 'buy':
            if confidence > 0.7:
                return f"é«˜ä¿¡å¿ƒåº¦({confidence:.1%})å»ºè­°è²·å…¥ï¼Œé æœŸæœªä¾†5å¤©å ±é…¬ç‡å¯èƒ½è¶…é3%"
            elif confidence > 0.5:
                return f"ä¸­ç­‰ä¿¡å¿ƒåº¦({confidence:.1%})å»ºè­°è²·å…¥ï¼Œé æœŸæœªä¾†5å¤©å¯èƒ½æœ‰æ­£å ±é…¬"
            else:
                return f"ä½ä¿¡å¿ƒåº¦({confidence:.1%})å»ºè­°è²·å…¥ï¼Œä½†éœ€è¬¹æ…è€ƒæ…®"
        elif action == 'sell':
            if confidence > 0.7:
                return f"é«˜ä¿¡å¿ƒåº¦({confidence:.1%})å»ºè­°è³£å‡ºï¼Œé æœŸæœªä¾†5å¤©å ±é…¬ç‡å¯èƒ½ä½æ–¼-3%"
            elif confidence > 0.5:
                return f"ä¸­ç­‰ä¿¡å¿ƒåº¦({confidence:.1%})å»ºè­°è³£å‡ºï¼Œé æœŸæœªä¾†5å¤©å¯èƒ½æœ‰è² å ±é…¬"
            else:
                return f"ä½ä¿¡å¿ƒåº¦({confidence:.1%})å»ºè­°è³£å‡ºï¼Œä½†éœ€è¬¹æ…è€ƒæ…®"
        else:  # hold
            return f"å»ºè­°æŒæœ‰({confidence:.1%}ä¿¡å¿ƒåº¦)ï¼Œé æœŸå ±é…¬ç‡åœ¨-3%åˆ°3%ä¹‹é–“"

    def rolling_validation(self, data: pd.DataFrame, 
                          initial_train_size: int = None,
                          retrain_frequency: int = 5) -> dict:  # 5å¤©
        """
        é€£çºŒæ»¾å‹•é©—è­‰ï¼ˆä¸åˆ†è¼ªæ¬¡ï¼‰
        
        Args:
            data: å®Œæ•´æ•¸æ“šé›†
            initial_train_size: åˆå§‹è¨“ç·´é›†å¤§å°ï¼ˆé è¨­ç‚ºç¸½æ•¸æ“šçš„60%ï¼‰
            retrain_frequency: é‡æ–°è¨“ç·´çš„é »ç‡ï¼ˆ5å¤©ï¼‰
        """
        self.logger.info("ğŸ”„ é–‹å§‹é€£çºŒæ»¾å‹•é©—è­‰...")
        self.logger.info("=" * 60)
        
        if initial_train_size is None:
            # é è¨­ä½¿ç”¨2019å¹´12æœˆ31æ—¥ä½œç‚ºè¨“ç·´é›†æˆªæ­¢æ—¥æœŸ
            cutoff_date = pd.to_datetime('2019-12-31')
            initial_train_size = len(data[data['Date'] <= cutoff_date])
        
        # ç¢ºä¿æœ‰è¶³å¤ çš„æ•¸æ“š
        min_required = initial_train_size + self.lookback_days
        if len(data) < min_required:
            raise ValueError(f"æ•¸æ“šä¸è¶³ï¼Œè‡³å°‘éœ€è¦ {min_required} è¡Œæ•¸æ“š")
        
        # æº–å‚™åˆå§‹è¨“ç·´æ•¸æ“š
        train_data = data[:initial_train_size]
        
        # é©—è­‰æ•¸æ“šæ˜¯å‰©é¤˜çš„æ‰€æœ‰æ•¸æ“š
        validation_start = initial_train_size
        validation_end = len(data) - self.lookback_days
        validation_data = data[validation_start:validation_end]
        
        self.logger.info(f"ğŸ“… è¨“ç·´æœŸé–“: {train_data['Date'].min()} - {train_data['Date'].max()}")
        self.logger.info(f"ğŸ“… é©—è­‰æœŸé–“: {validation_data['Date'].min()} - {validation_data['Date'].max()}")
        self.logger.info(f"ğŸ“Š è¨“ç·´æ¨£æœ¬æ•¸: {len(train_data)}, é©—è­‰æ¨£æœ¬æ•¸: {len(validation_data)}")
        
        # åˆå§‹è¨“ç·´æ¨¡å‹
        self.logger.info("ğŸš€ åˆå§‹è¨“ç·´æ¨¡å‹...")
        model_perf = self.train(train_data)
        
        # é€²è¡Œé€£çºŒæ»¾å‹•é©—è­‰ï¼Œæ¯é€±ä¸‰é‡æ–°è¨“ç·´
        print(f"é–‹å§‹é€£çºŒæ»¾å‹•é©—è­‰ï¼Œå…± {len(validation_data)} å¤©...")
        validation_results = self._validate_with_frequent_retrain(
            validation_data, 
            initial_train_size, 
            retrain_frequency, 
            data
        )
        
        # æº–å‚™çµæœçµæ§‹
        validation_results.update({
            'train_start': train_data['Date'].min().isoformat(),
            'train_end': train_data['Date'].max().isoformat(),
            'validation_start': validation_data['Date'].min().isoformat(),
            'validation_end': validation_data['Date'].max().isoformat(),
            'train_size': len(train_data),
            'validation_size': len(validation_data)
        })
        
        # ä¿å­˜æ¯æ—¥è©³ç´°ç‹€æ…‹ CSV
        if 'daily_results' in validation_results:
            daily_df = validation_results['daily_results'].copy()
            
            # è¨­ç½®æ‚¨è¦æ±‚çš„æ¬„ä½é †åº
            columns_order = [
                'date',           # æ—¥æœŸ
                'open',           # é–‹ç›¤
                'close',          # æ”¶ç›¤
                'predicted_action', # å‹•ä½œ
                'prob_buy',       # è²·å…¥æ©Ÿç‡
                'prob_hold',      # æŒæœ‰æ©Ÿç‡
                'prob_sell'       # è³£å‡ºæ©Ÿç‡
            ]
            
            # ç¢ºä¿æ‰€æœ‰æ¬„ä½éƒ½å­˜åœ¨
            for col in columns_order:
                if col not in daily_df.columns:
                    daily_df[col] = None
                    
            daily_df = daily_df[columns_order]
            
            # åœ¨é‡æ–°å‘½åå‰è¨ˆç®—çµ±è¨ˆè³‡è¨Š
            if 'is_wednesday' in validation_results['daily_results'].columns:
                original_df = validation_results['daily_results']
                wednesday_retrains = original_df[original_df['is_wednesday'] == True]
                wednesday_count = len(wednesday_retrains)
            else:
                wednesday_count = validation_results.get('retrain_count', 0)
            
            # é‡æ–°å‘½åæ¬„ä½ç‚ºä¸­æ–‡
            column_rename_map = {
                'date': 'æ—¥æœŸ',
                'open': 'é–‹ç›¤',
                'close': 'æ”¶ç›¤',
                'predicted_action': 'å‹•ä½œ',
                'prob_buy': 'è²·å…¥æ©Ÿç‡',
                'prob_hold': 'æŒæœ‰æ©Ÿç‡',
                'prob_sell': 'è³£å‡ºæ©Ÿç‡'
            }
            daily_df = daily_df.rename(columns=column_rename_map)
            
            # ä¿å­˜æ¯æ—¥è©³ç´°ç‹€æ…‹ CSV
            csv_filename = 'log/rolling_validation_daily_details.csv'
            daily_df.to_csv(csv_filename, index=False, encoding='utf-8-sig')
            
            self.logger.info(f"æ¯æ—¥è©³ç´°ç‹€æ…‹å·²ä¿å­˜åˆ°: {csv_filename}")
            print(f"\næ¯æ—¥è©³ç´°ç‹€æ…‹å·²ä¿å­˜åˆ°: {csv_filename}")
            print(f"ç¸½å…± {len(daily_df)} å¤©çš„è©³ç´°è¨˜éŒ„")
            
            # é¡¯ç¤ºä¸€äº›çµ±è¨ˆè³‡è¨Š
            print(f"é€±ä¸‰é‡æ–°è¨“ç·´æ¬¡æ•¸: {wednesday_count} æ¬¡")
        
        self.logger.info("ğŸ‰ é€£çºŒæ»¾å‹•é©—è­‰å®Œæˆ")
        self.logger.info("=" * 60)
        
        # æº–å‚™ç´”é æ¸¬é©—è­‰æ‘˜è¦
        summary = {
            'prediction_accuracy': validation_results.get('prediction_accuracy', 0),
            'action_accuracy': validation_results.get('action_accuracy', {}),
            'avg_confidence': validation_results.get('avg_confidence', 0),
            'total_predictions': validation_results.get('total_predictions', 0),
            'correct_predictions': validation_results.get('correct_predictions', 0),
            'retrain_count': validation_results.get('retrain_count', 0)
        }
        
        return {
            'summary': summary,
            'detailed_results': [validation_results],  # åŒ…è£æˆåˆ—è¡¨ä»¥ä¿æŒä¸€è‡´æ€§
            'validation_config': {
                'initial_train_size': initial_train_size,
                'retrain_frequency': 'æ¯é€±ä¸‰',
                'total_validation_days': len(validation_data)
            },
            'daily_results_csv': csv_filename if 'daily_results' in validation_results else None
        }
    
    def _validate_with_frequent_retrain(self, validation_data: pd.DataFrame, 
                                       train_end_position: int, 
                                       retrain_frequency: int, 
                                       full_data: pd.DataFrame) -> dict:
        """åœ¨é©—è­‰æœŸé–“å…§é€²è¡Œé æ¸¬é©—è­‰ï¼Œä¸¦åœ¨æ¯é€±ä¸‰é‡æ–°è¨“ç·´æ¨¡å‹"""
        results = []
        retrain_count = 0
        
        # åœ¨é©—è­‰æœŸé–“å…§é€æ—¥é æ¸¬
        days_to_process = len(validation_data) - self.lookback_days
        with tqdm(total=days_to_process, desc="æ¯æ—¥é æ¸¬é©—è­‰", unit="å¤©", leave=False) as day_pbar:
            for i in range(days_to_process):
                # ç•¶å‰é©—è­‰æ—¥çš„æ•¸æ“š
                row = validation_data.iloc[i]
                current_date = pd.to_datetime(row['Date'])
                current_close = row['Close']
                
                day_pbar.set_description(f"é©—è­‰æ—¥: {current_date.strftime('%Y-%m-%d')}")
                
                # æª¢æŸ¥æ˜¯å¦ç‚ºé€±ä¸‰ï¼ˆweekday() == 2ï¼Œé€±ä¸€=0ï¼Œé€±äºŒ=1ï¼Œé€±ä¸‰=2ï¼‰
                if current_date.weekday() == 2 and i > 0:  # é€±ä¸‰ä¸”ä¸æ˜¯ç¬¬ä¸€å¤©
                    retrain_count += 1
                    # ä½¿ç”¨åˆ°ç•¶å‰æ™‚é–“é»çš„æ‰€æœ‰æ•¸æ“šé‡æ–°è¨“ç·´
                    retrain_end = train_end_position + i
                    retrain_data = full_data[:retrain_end]
                    day_pbar.set_description(f"é€±ä¸‰é‡è¨“: {current_date.strftime('%Y-%m-%d')}")
                    self.logger.info(f"ğŸ”„ é€±ä¸‰é‡æ–°è¨“ç·´ (ç¬¬ {retrain_count} æ¬¡): {current_date.strftime('%Y-%m-%d')}, æ•¸æ“šç¯„åœ: ç¬¬1-{retrain_end}è¡Œ")
                    try:
                        self.train(retrain_data)
                        self.logger.info(f"âœ… ç¬¬ {retrain_count} æ¬¡é‡è¨“å®Œæˆ")
                    except Exception as e:
                        self.logger.error(f"âŒ é‡æ–°è¨“ç·´å¤±æ•—: {e}")
                
                # ç”Ÿæˆç•¶å¤©çš„é æ¸¬è¨Šè™Ÿ
                try:
                    prediction = self.predict(row)
                    action = prediction['action']
                    confidence = prediction['confidence']
                except Exception as e:
                    self.logger.warning(f"âš ï¸ é æ¸¬å¤±æ•—: {e}")
                    action = 'hold'
                    confidence = 0.0
                    prediction = {'probabilities': {'buy': 0.33, 'hold': 0.34, 'sell': 0.33}}
                
                # è¨ˆç®—å¯¦éš›æœªä¾†å ±é…¬ç‡ï¼ˆç”¨æ–¼è©•ä¼°é æ¸¬æº–ç¢ºæ€§ï¼‰
                actual_return = self._calculate_future_return(validation_data, i)
                if actual_return is None:
                    day_pbar.update(1)
                    continue
                
                actual_action = self._classify_return(actual_return)
                
                results.append({
                    'date': current_date.strftime('%Y-%m-%d'),
                    'open': row.get('Open', current_close),  # é–‹ç›¤åƒ¹ï¼Œå¦‚æœæ²’æœ‰å‰‡ç”¨æ”¶ç›¤åƒ¹
                    'close': current_close,  # æ”¶ç›¤åƒ¹
                    'predicted_action': action,
                    'prob_buy': f"{prediction['probabilities'].get('buy', 0) * 100:.2f}%",
                    'prob_hold': f"{prediction['probabilities'].get('hold', 0) * 100:.2f}%",
                    'prob_sell': f"{prediction['probabilities'].get('sell', 0) * 100:.2f}%",
                    # ä¿ç•™å…¶ä»–æ¬„ä½ä»¥å‚™ä¸æ™‚ä¹‹éœ€
                    'weekday': current_date.strftime('%A'),
                    'is_wednesday': current_date.weekday() == 2,
                    'actual_action': actual_action,
                    'confidence': confidence,
                    'actual_return': actual_return,
                    'retrain_count': retrain_count
                })
                
                # æ›´æ–°é€²åº¦æ¢
                day_pbar.update(1)
                day_pbar.set_postfix({
                    'é æ¸¬': action,
                    'å¯¦éš›': actual_action,
                    'ä¿¡å¿ƒ': f"{confidence:.1%}"
                })
        
        # è¨ˆç®—ç´”é æ¸¬é©—è­‰æŒ‡æ¨™
        df_results = pd.DataFrame(results)
        if len(df_results) == 0:
            return self._empty_validation_result()
        
        # åªè¨ˆç®—é æ¸¬æº–ç¢ºç‡ç›¸é—œæŒ‡æ¨™
        correct_predictions = (df_results['predicted_action'] == df_results['actual_action']).sum()
        prediction_accuracy = correct_predictions / len(df_results) if len(df_results) > 0 else 0
        
        # æŒ‰å‹•ä½œåˆ†é¡æº–ç¢ºç‡
        action_accuracy = {}
        for action in ['buy', 'hold', 'sell']:
            action_mask = df_results['actual_action'] == action
            if action_mask.sum() > 0:
                action_correct = ((df_results['predicted_action'] == action) & action_mask).sum()
                action_accuracy[action] = action_correct / action_mask.sum()
            else:
                action_accuracy[action] = 0.0
        
        return {
            'prediction_accuracy': prediction_accuracy,
            'action_accuracy': action_accuracy,
            'avg_confidence': df_results['confidence'].mean(),
            'total_predictions': len(df_results),
            'correct_predictions': correct_predictions,
            'retrain_count': retrain_count,
            'daily_results': df_results
        }
    
    def _empty_validation_result(self) -> dict:
        """è¿”å›ç©ºçš„é©—è­‰çµæœ"""
        return {
            'prediction_accuracy': 0.0,
            'action_accuracy': {'buy': 0.0, 'hold': 0.0, 'sell': 0.0},
            'avg_confidence': 0.0,
            'total_predictions': 0,
            'correct_predictions': 0,
            'retrain_count': 0,
            'daily_results': pd.DataFrame()
        }
    
    def save_model(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        Path(filepath).parent.mkdir(parents=True, exist_ok=True)
        
        model_data = {
            'model': self.model,
            'feature_columns': self.feature_columns,
            'label_encoders': self.label_encoders,
            'class_labels': self.class_labels,
            'inverse_labels': self.inverse_labels,
            'lookback_days': self.lookback_days,
            'return_threshold_buy': self.return_threshold_buy,
            'return_threshold_sell': self.return_threshold_sell,
            'evaluation_results': getattr(self, 'evaluation_results', {})
        }
        
        joblib.dump(model_data, filepath)
        self.logger.info(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {filepath}")
    
    def load_model(self, filepath: str):
        """è¼‰å…¥æ¨¡å‹"""
        if not Path(filepath).exists():
            raise FileNotFoundError(f"æ¨¡å‹æª”æ¡ˆä¸å­˜åœ¨: {filepath}")
        
        model_data = joblib.load(filepath)
        
        self.model = model_data['model']
        self.feature_columns = model_data['feature_columns']
        self.label_encoders = model_data['label_encoders']
        self.class_labels = model_data['class_labels']
        self.inverse_labels = model_data['inverse_labels']
        self.lookback_days = model_data['lookback_days']
        self.return_threshold_buy = model_data['return_threshold_buy']
        self.return_threshold_sell = model_data['return_threshold_sell']
        self.evaluation_results = model_data.get('evaluation_results', {})
        
        self.logger.info(f"æ¨¡å‹å·²å¾ {filepath} è¼‰å…¥")

def run_rolling_validation():
    """åŸ·è¡Œæ»¾å‹•é©—è­‰çš„ä¸»å‡½æ•¸"""
    # è¼‰å…¥æ•¸æ“š
    print("è¼‰å…¥æ•¸æ“š...")
    data = pd.read_csv('data/final_data.csv')
    data['Date'] = pd.to_datetime(data['Date'])
    data = data.sort_values('Date').reset_index(drop=True)
    
    print(f"æ•¸æ“šå½¢ç‹€: {data.shape}")
    print(f"æ—¥æœŸç¯„åœ: {data['Date'].min()} åˆ° {data['Date'].max()}")
    
    # å‰µå»ºåˆ†é¡å™¨
    classifier = BayesianStateClassifier(lookback_days=5)
    
    # åŸ·è¡Œæ»¾å‹•é©—è­‰
    print("\n=== åŸ·è¡Œé€£çºŒé æ¸¬é©—è­‰ï¼ˆé€±ä¸‰é‡æ–°è¨“ç·´ï¼‰===")
    
    # é…ç½®æ»¾å‹•é©—è­‰åƒæ•¸
    # è¨­å®šåˆå§‹è¨“ç·´é›†åˆ°2019å¹´12æœˆ31æ—¥
    cutoff_date = pd.to_datetime('2019-12-31')
    initial_train_size = len(data[data['Date'] <= cutoff_date])
    retrain_frequency = 'Wednesday'  # æ¯é€±ä¸‰é‡æ–°è¨“ç·´

    print(f"åˆå§‹è¨“ç·´é›†å¤§å°: {initial_train_size} å¤©")
    print(f"é‡æ–°è¨“ç·´é »ç‡: æ¯é€±ä¸‰")
    
    # åŸ·è¡Œé€£çºŒæ»¾å‹•é©—è­‰
    rolling_results = classifier.rolling_validation(
        data, 
        initial_train_size=initial_train_size,
        retrain_frequency=7  # å‚³å…¥7å¤©é »ç‡ï¼ˆå¯¦éš›ä¸Šæœƒè¢«é€±ä¸‰é‚è¼¯è¦†è“‹ï¼‰
    )
    
    # é¡¯ç¤ºé æ¸¬é©—è­‰çµæœ
    summary = rolling_results['summary']
    print("\n=== é€£çºŒé æ¸¬é©—è­‰çµæœæ‘˜è¦ ===")
    print(f"é æ¸¬æº–ç¢ºç‡: {summary['prediction_accuracy']:.2%}")
    print(f"å¹³å‡ä¿¡å¿ƒåº¦: {summary['avg_confidence']:.2%}")
    print(f"ç¸½é æ¸¬æ¬¡æ•¸: {summary['total_predictions']}")
    print(f"æ­£ç¢ºé æ¸¬æ¬¡æ•¸: {summary['correct_predictions']}")
    print(f"é‡æ–°è¨“ç·´æ¬¡æ•¸: {summary['retrain_count']}")
    print(f"ç¸½é©—è­‰å¤©æ•¸: {rolling_results['validation_config']['total_validation_days']}")
    
    # é¡¯ç¤ºå„å‹•ä½œé æ¸¬æº–ç¢ºç‡
    action_acc = summary['action_accuracy']
    print(f"\nå„å‹•ä½œé æ¸¬æº–ç¢ºç‡:")
    print(f"  è²·å…¥ (buy): {action_acc.get('buy', 0):.2%}")
    print(f"  æŒæœ‰ (hold): {action_acc.get('hold', 0):.2%}")
    print(f"  è³£å‡º (sell): {action_acc.get('sell', 0):.2%}")
    
    # ä¿å­˜æ»¾å‹•é©—è­‰çµæœ
    with open('log/rolling_validation_results.json', 'w', encoding='utf-8') as f:
        json.dump(rolling_results, f, ensure_ascii=False, indent=2, default=str)
    
    print(f"\né æ¸¬é©—è­‰çµæœå·²ä¿å­˜åˆ°: log/rolling_validation_results.json")
    
    # é¡¯ç¤ºé©—è­‰çµæœè©³ç´°è³‡è¨Š
    print(f"\n=== é©—è­‰è©³ç´°çµæœ ===")
    result = rolling_results['detailed_results'][0]  # åªæœ‰ä¸€å€‹çµæœ
    print(f"é©—è­‰æœŸé–“: {result['validation_start'][:10]} - {result['validation_end'][:10]}")
    print(f"é æ¸¬æº–ç¢ºç‡: {result['prediction_accuracy']:.2%}")
    print(f"å¹³å‡ä¿¡å¿ƒåº¦: {result['avg_confidence']:.2%}")
    print(f"ç¸½é æ¸¬æ¬¡æ•¸: {result['total_predictions']}")
    print(f"é‡æ–°è¨“ç·´æ¬¡æ•¸: {result['retrain_count']}")
    
    # ç”¨æœ€æ–°æ•¸æ“šé‡æ–°è¨“ç·´æœ€çµ‚æ¨¡å‹
    print(f"\n=== è¨“ç·´æœ€çµ‚æ¨¡å‹ ===")
    final_train_size = int(len(data) * 0.8)
    final_train_data = data[:final_train_size]
    evaluation_results = classifier.train(final_train_data)
    
    # ä¿å­˜æœ€çµ‚æ¨¡å‹
    classifier.save_model('log/bayesian_classifier_model.pkl')
    
    # ç”Ÿæˆæ˜å¤©çš„äº¤æ˜“è¨Šè™Ÿ
    print("\n=== æ˜å¤©çš„äº¤æ˜“è¨Šè™Ÿ ===")
    latest_data = data.iloc[-1]
    tomorrow_signal = classifier.get_next_day_signal(latest_data)
    
    print(f"è¨Šè™Ÿç”¢ç”Ÿæ—¥æœŸ: {latest_data['Date'].strftime('%Y-%m-%d')}")
    print(f"åŸ·è¡Œäº¤æ˜“æ—¥æœŸ: {(latest_data['Date'] + pd.Timedelta(days=1)).strftime('%Y-%m-%d')}")
    print(f"ç•¶å‰æ”¶ç›¤åƒ¹: ${latest_data['Close']:.2f}")
    if 'Open' in data.columns:
        print(f"ç•¶å‰é–‹ç›¤åƒ¹: ${latest_data['Open']:.2f}")
    print(f"æ˜å¤©å»ºè­°å‹•ä½œ: {tomorrow_signal['recommended_action']}")
    print(f"ä¿¡å¿ƒåº¦: {tomorrow_signal['confidence']:.2%}")
    print(f"æ¨ç†èªªæ˜: {tomorrow_signal['reasoning']}")
    print("å‹•ä½œæ¦‚ç‡:")
    for action, prob in tomorrow_signal['action_probabilities'].items():
        print(f"  {action}: {prob:.2%}")
    
    # ä¿å­˜æ˜å¤©çš„è¨Šè™Ÿ
    with open('log/tomorrow_trading_signal.json', 'w', encoding='utf-8') as f:
        json.dump(tomorrow_signal, f, ensure_ascii=False, indent=2)
    
    print(f"\næ˜å¤©çš„äº¤æ˜“è¨Šè™Ÿå·²ä¿å­˜åˆ°: log/tomorrow_trading_signal.json")


def main():
    """ä¸»å‡½æ•¸ - åªåŸ·è¡Œæ»¾å‹•é©—è­‰"""
    run_rolling_validation()


if __name__ == "__main__":
    main()